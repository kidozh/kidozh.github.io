<!doctype html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <title>Digital Twins of Tool Wear by Deep Convolutional Generative Adversarial Network</title>
    <!-- customize css-->
    <link rel="stylesheet" href="./static/css/style.css">
</head>
<body>

<div class="container-fluid">
    <div class="row project-portal">
        <div class="col-md-10 offset-md-1 project-intro">
            <h1 class="text-center project-title">Digital Twins of Machining Tool</h1>
            <p class="text-center sub-title">By Deep Conditional Convolutional Generative Adversial Network</p>
            <p class="text-center"><a class="btn">Read Paper</a> <a href="https://github.com/kidozh/digital_twin_by_GAN" class="btn btn-inverse">Project &gt;</a> </p>
        </div>

    </div>
    <div class="row project-feature">
        <div class="container">
            <div class="row">
                <div class="col-md-4">
                    <h2 class="title">Unsupervised</h2>
                    <p class="paragraph">DCGAN is an advanced unsupervised approach to clone the distribution of real signal data.
                        Upsampling and Convolutional layer constitute transpose convolutional layer to generate hierarchy signal blueprint.
                        By competition between discriminator and generator, it's easy to re-create realistic signal.
                    </p>
                    <p class="paragraph">Since the data is obtained by unsupervised approach, the final result could be not stable.
                    In addition, because the optimization point is located at one saddle point of G-D tensor space, it make optimization
                        even harder.
                    </p>
                </div>
                <div class="col-md-4">
                    <h2 class="title">End-to-end Design</h2>
                    <p class="paragraph">
                        Key to simplify model's design is to using end-to-end architecture. The hybrid of generative and discriminative network
                        allows model to simplify both train and application process.
                    </p>
                    <p class="paragraph">
                        Fed with high-dimensional random noise, GAN can give diverse signal sample which is hard for machine to recognize validity.
                        By progressive deep-learning training, generative network is able to evolve itself. A remarkable signal could be obtained after
                        proper epoch of deep learning training.
                    </p>
                </div>
                <div class="col-md-4">
                    <h2 class="title">General</h2>
                    <p class="paragraph">
                        There is no requirement for distribution of original data. It enable the widespread use of GAN in general circumstances. The
                        only thing GAN need to do is clone the siblings of signal data. By introducing lantent space parameter(z), GAN can generate the
                        corresponding signal. It could help us to reinforce data, diagnose failure and optimize the best machining parameters by searching
                        z tensor space. It can be the fundamental ground research which enable network to handle realtime machining just like AlphaGo.
                    </p>
                </div>
            </div>

        </div>

    </div>
    <div class="row">
        <div class="container">
            <div class="row project-principle">
                <div class="col-md-12">
                    <h1 class="title">Co-evolution</h1>
                    <p>GAN training implements game theory that makes G and D network compete to exploit their performance.
                    Generative network(G) tries to generate realistic signal to deceive discriminative network(D). D tries to
                        identify the validity of given signal. Finally, both network achieve <strong>nash balance</strong> which means discriminator
                        fails to recognize signal and generator can give sequence whose data distribution is the same as original sample.
                    </p>
                    <h4 class="sub-title">Nash balance</h4>
                    <p>
                        Since the limit of sample's size and optimization's location, training of GAN is hard. Also, because range in every dimension is
                        different, the compound tensor may not be so accurate. But the tendency shows that GAN can clone data well.

                    </p>
                </div>
                <div class="col-md-12">
                    <p class="text-center"><img class="img-fluid" src="./static/images/gan_principle.svg"> </p>
                    <p class="text-center caption">The architecture of Generative Adversial Network(GAN) and digital twins' relation to it</p>
                </div>
            </div>

            <div class="row component">
                <div class="col-md-8">
                    <h1 class="title">Discriminative Network</h1>
                    <p>
                        The core of discriminator is classical post-activation design. By MLP architecture, discriminator can extract
                        features of machining signal at every abstract level.
                    </p>
                    <p>
                        The discriminator is fed with signal which may comes from real data and fake data generated by G-network. This
                        network will output a binary value recognizing validity of given sequence. Trained with Adam optimization
                        (learning rate 0.0001 and \( beta_1 \) 0.5), the discriminator will adapt itself to identify signal.
                    </p>
                    <h3 class="title">Loss function</h3>
                    <p>The loss function is binary crossentropy which is used for binary recognition.</p>
                    <p>
                        $$  loss = -\sum_{i=1}^{n}\hat{y_i}logy_i+(1-\hat{y_i})log(1-y_i)\\ \frac {\partial loss} {\partial y} = -\sum_{i=1}^{n}\frac {\hat{y_i}} {y_i}-\frac {1-\hat{y_i}} {1-y_i} $$
                    </p>

                    <h3 class="title">Technical Tricks</h3>
                    <p> In order to prevent sparse features, <strong>leaky rectified linear unit</strong>(LeakyRelu) activation is used.
                        In practical training of GAN, discriminator can always easily recognize whether given signal is true or fake. In order
                        to make competition stable, dropout layer with fair large dropout rate is applied. Because of prior success in monitoring
                        realtime tool wear, a descending dimension is implement in convolutional layer.
                    </p>

                </div>
                <div class="col-md-4">
                    <p class="text-center"><img class="img-fluid" src="./static/images/discriminative_network_architecture.svg"> </p>
                    <p class="caption text-center">The discriminator's architecture. (This architecture may change according to latest development
                        progress)</p>
                </div>
            </div>
            <div class="row component">

                <div class="col-md-8">
                    <h1 class="title">Generative Network</h1>
                    <p>
                        The core of generator is <strong><span class="upsampling">upsampling</span>-<span class="conv1d">convolutional</span></strong>
                        deconvolution structure. Upsampling enables signal to reshape and convolution allows information fusion at different time
                        of sequence. InstanceNormalization can better normalize features and avoid sparse characteristics in generative model.
                        LeakyRelu as the non-sparse activation function can reduce features' redundancy.
                    </p>
                    <p>
                        The generator is fed with signal which combined both noise(Usually <span class="text-success">Gaussian distribution</span>) and machining condition(need rescale method).
                        This network will yield a signal corresponding to given machining condition. Trained with Adam optimization (learning rate
                        0.0001 and \( beta_1 \) 0.5), the generator will adapt itself to produce signal according to machining condition.
                    </p>
                    <h3 class="title">Rescale method</h3>
                    <p>
                        For different machining condition, a separate rescaling method should be taken for better GAN training.
                    </p>
                    <ul class="machining-condition">
                        <li>
                            <h5 class="title">For discrete machining condition</h5>
                            <p>
                                For many machining condition, the change is not continuous. For example, machining tool type, workpiece material and
                                milling approach could all be considered discrete status. In this case, <b>embedding layer</b> should be taken into
                                use. This layer can implicitly indicates the relation among different status and decrease calculation cost. In
                                addition, if machining condition is complex and representing tensor is high dimensional, the sparse vector will increase
                                unnecessary cost and impede further immigration to new machining condition.
                            </p>
                            <p>
                                When network is trained, all embedding tensor will be updated. By calculating the similarity of different status,
                                many features could be reused to increase accuracy and generality. It's also helpful when later transfer learning
                                is applied.
                            </p>
                        </li>
                        <li>
                            <h5 class="title">For continuous machining condition</h5>
                            <p>
                                Continuous variables are also common in processing workpieces. For example, spindle speed, feed rate, machining temperature
                                and cut depth could set at any given figure. In this case, <b>full connected layer</b> should be used to rescale parameters.
                                In this layer, it can be simplified as a linear transformation \( y = a \times x + b \). If you know exact proper scaling
                                methods, it's better to directly transform the variable before training model.
                            </p>
                        </li>

                    </ul>
                    <p>Later, processed machining factor will be multiplied with rescale factor as the input of generator. This is the reason why
                        DCGAN is prefixed with <strong class="highlight-text">C</strong> which indicates conditional.
                    </p>
                    <h3 class="title">Technical tricks</h3>
                    <p>
                        The techniques applied in generative model are the same as discriminative one.
                    </p>
                </div>
                <div class="col-md-4">
                    <p class="text-center"><img class="img-fluid" src="./static/images/generative_network_architecture.svg"> </p>
                    <p class="text-center caption">
                        The generator's architecture(This architecture may change according to latest development progress).
                        <span class="text-danger">×</span> rounded by red dot means <span class="text-danger">multiply</span>
                        operation.
                    </p>
                </div>
            </div>
            <div class="row component">
                <div class="col-md-6">
                    <h1 class="title">Training Process</h1>
                    <p>The ideal mode of training is that G and D network compete to achieve nash balance. The best condition is
                     that accuracy of D is fluctuated around 50%. However, due to some reasons we have hardly known, D's accuracy is
                        unusual high which means D is very smart to recognize signal's validity. Even we increase dropout and G's
                        complexity, the phenomenon is still not apparent to see. But after exam generative signal with high-accurate ResNet,
                        the signal could be accepted in a way. Also, by observing final signal sample, it seems like experimental
                        signal.
                    </p>

                </div>
                <div class="col-md-6">
                    <p class="text-center"><img class="img-fluid" src="./static/images/loss_change_example.png"> </p>
                    <p class="text-center caption">
                        The training example of GAN. <span class="text-danger">The example is not related to this experiment!</span>
                    </p>
                </div>
            </div>
            <div class="row component">
                <div class="col-md-6">
                    <p class="text-center">
                        <img src="./static/images/DCGAN_57000_epoch.svg" class="img-fluid">
                    </p>
                    <p class="text-center caption">
                        The generated signal after 57000 epoch evaluated by ResNet. The tendency shows that generator can simulate tool wear tendency well.
                        Due to my error, legend of this graph is not given. The <span class="text-info">x-axis</span> is
                        <span class="text-info">given tool wear</span> and <span class="text-info">y-axis</span> is
                        <span class="text-info">estimated value</span> examined by ResNet.

                    </p>
                </div>
            </div>
        </div>
    </div>

</div>


<!-- Optional JavaScript -->
<!-- jQuery first, then Popper.js, then Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
<!-- mathjax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

</body>
</html>