<!DOCTYPE html>




<html class="theme-next pisces" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="kidozh的一些想法和随手笔记">
<meta property="og:type" content="website">
<meta property="og:title" content="kidozh">
<meta property="og:url" content="https://kidozh.github.io/page/2/index.html">
<meta property="og:site_name" content="kidozh">
<meta property="og:description" content="kidozh的一些想法和随手笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="kidozh">
<meta name="twitter:description" content="kidozh的一些想法和随手笔记">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: false,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://kidozh.github.io/page/2/"/>





  <title>kidozh</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">kidozh</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">某不科学的kidozh</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://kidozh.github.io/2017/06/18/暑假安排/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="kido zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="kidozh">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/06/18/暑假安排/" itemprop="url">暑假安排</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-18T21:43:34+08:00">
                2017-06-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>鉴于暑假时间特别的长，再加上我的个人原因，我写了一个大致的计划来规划我暑假的生活。</p>
<p>暑假的安排主要还是面向以后的方向，在暑假主要是查阅文献并整理出一个文献的报告。</p>
<h1 id="时间长度"><a href="#时间长度" class="headerlink" title="时间长度"></a>时间长度</h1><p>按照学校的安排，我主要还是按照一周来计划要做的事情</p>
<h1 id="周内安排"><a href="#周内安排" class="headerlink" title="周内安排"></a>周内安排</h1><p>在最终期限的早上，我会把我的一些对于本周的阅读的文献还有心得体会写到我的<a href="https://kidozh.github.io">个人博客</a>里，同时也力求提出一些<code>point</code>来启发我进行接下来的阅读</p>
<h1 id="每周安排"><a href="#每周安排" class="headerlink" title="每周安排"></a>每周安排</h1><table>
<thead>
<tr>
<th>起始时间</th>
<th>结束时间</th>
<th>安排</th>
</tr>
</thead>
<tbody>
<tr>
<td>6.17</td>
<td>6.24</td>
<td><code>数控技术与制造自动化</code>的第2章的习题和第三章</td>
<td></td>
</tr>
<tr>
<td>6.25</td>
<td>7.1</td>
<td><code>数控技术与制造自动化</code>的第三章及习题</td>
<td></td>
</tr>
<tr>
<td>7.2</td>
<td>7.8</td>
<td><code>数控技术与制造自动化</code>4,5章</td>
<td></td>
</tr>
<tr>
<td>7.9</td>
<td>7.15</td>
<td><code>数控技术与制造自动化</code>第6章以及在附录中的一些算法以及全书小结</td>
<td></td>
</tr>
<tr>
<td>7.15</td>
<td>7.22</td>
<td>看论文（至少7+1篇）</td>
<td></td>
</tr>
<tr>
<td>7.22</td>
<td>7.29</td>
<td>看论文（至少7+1篇）</td>
<td></td>
</tr>
<tr>
<td>7.29</td>
<td>8.5</td>
<td>看论文（至少7+1篇）</td>
<td></td>
</tr>
<tr>
<td>8.5</td>
<td>8.12</td>
<td>看论文（至少7+1篇）</td>
<td></td>
</tr>
<tr>
<td>8.12</td>
<td>8.19</td>
<td>看论文（至少7+1篇）</td>
<td></td>
</tr>
<tr>
<td>8.19</td>
<td>学期开始</td>
<td>看论文（至少10+3篇）</td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>如果有变动，我会在博客里说明。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://kidozh.github.io/2017/05/31/一周小结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="kido zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="kidozh">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/05/31/一周小结/" itemprop="url">一周小结</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-05-31T19:26:51+08:00">
                2017-05-31
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在上周，我看了一些关于在线测量的论文，现在我说一下我对论文的概括和一些想法。</p>
<h1 id="平面立铣表面纹理研究"><a href="#平面立铣表面纹理研究" class="headerlink" title="平面立铣表面纹理研究"></a>平面立铣表面纹理研究</h1><h2 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h2><ul>
<li>刀刃转移到工件的能量越高，工件表面缺陷形成的可能就越大</li>
</ul>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul>
<li>在速度较低的情况下，切削速度越大，就会导致温度随之升高，这样就会使得摩擦力下降，从而使得表面纹理的特征下降。在速度较高的情况下，刀具就会发生激振，从而致使表面纹理特征值升高。此时的激振并不是共振。</li>
<li>随着加工倾角的减小，此时工件表面处于冷硬摩擦的状态，接触点的速度也会降低，同事使得刮擦挤压变形变大，磨损增加，致使表面纹理的状态变差。实验表明，当倾角是<strong>15</strong>度的时候，加工效果最好。</li>
<li>表面纹理和进给量之间成反相关关系</li>
<li>前刀面与被加工工件的金属粘附，形成切削瘤，在加工过程之中，应该尽量避免切削瘤的产生</li>
<li>前刀面摩擦使得切削挤压工件，此时会形成鳞刺，使得加工条件变得恶化</li>
<li>随着刀刃的钝圆半径增大，使得工件表面温度升高，此时就会出现工件被灼烧这种情况产生</li>
</ul>
<h2 id="评价表面纹理的标准"><a href="#评价表面纹理的标准" class="headerlink" title="评价表面纹理的标准"></a>评价表面纹理的标准</h2><ul>
<li>纹理的粗细程度</li>
<li>纹理的分布均匀程度</li>
<li>纹理的相似度</li>
<li>纹理的局部变化程度</li>
<li>表面光洁度</li>
</ul>
<h2 id="加工信号的提取"><a href="#加工信号的提取" class="headerlink" title="加工信号的提取"></a>加工信号的提取</h2><h3 id="希尔伯特黄变换"><a href="#希尔伯特黄变换" class="headerlink" title="希尔伯特黄变换"></a>希尔伯特黄变换</h3><p>优化：</p>
<ul>
<li>与FFT不同，HHT可以避免假频和虚假信号的出现</li>
<li>无法分析局部特征</li>
<li>由于基函数固定，其自适应较差</li>
<li>能精确的描述时间和频率变化的关系</li>
</ul>
<h3 id="IMF振幅特征的提取"><a href="#IMF振幅特征的提取" class="headerlink" title="IMF振幅特征的提取"></a>IMF振幅特征的提取</h3><p>可以计算瞬时的频率。能分解出<strong>IMF</strong>的条件：</p>
<ol>
<li>上下包络线相对于时间轴对称</li>
<li>信号中极值点的个数和零点的个数相等或者相差个数不超过1</li>
</ol>
<h3 id="EMD"><a href="#EMD" class="headerlink" title="EMD"></a>EMD</h3><p>EMD即经验模态分解</p>
<p>假设信号均是由不同的IMF分量所组成，其具有相同数量的零点以及极值点</p>
<p>可以通过<strong>微分 -&gt; 分解 -&gt; 积分</strong>的方法补充数值点，这样就能获得IMF了。</p>
<p>极值点之间的时间间隔可以确定信号的时间尺度</p>
<h2 id="表面纹理分析"><a href="#表面纹理分析" class="headerlink" title="表面纹理分析"></a>表面纹理分析</h2><p>其依赖灰度共生矩阵</p>
<ul>
<li>ASM</li>
<li>逆差矩</li>
<li>熵</li>
<li>相关性</li>
<li>对比度</li>
</ul>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>None</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://kidozh.github.io/2017/05/12/入坑seq2seq模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="kido zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="kidozh">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/05/12/入坑seq2seq模型/" itemprop="url">入坑seq2seq模型</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-05-12T13:27:27+08:00">
                2017-05-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>递归神经网络（recurrent neural network）能够构建模型。这引发了一个很有趣的问题，我们可以对一些输入的单词进行调整，并产生有意义的响应吗？例如，我们可以训练一个神经网络来从英语翻译成法语吗？答案是可以的。</p>
<p>从Github上clone<a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="external">这个</a>，键入下面的命令就可以开始训练翻译模型了</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> models/tutorials/rnn/trans2late</div><div class="line">python translate.py --data_dir [your_data_directory]</div></pre></td></tr></table></figure>
<p>它将从<a href="http://www.statmt.org/wmt15/translation-task.html" target="_blank" rel="external">WMT’15</a>网站下载英文到法文的翻译数据 ，准备训练和训练。大约需要20GB的磁盘空间。</p>
<h1 id="Sequence-to-sequence基础"><a href="#Sequence-to-sequence基础" class="headerlink" title="Sequence-to-sequence基础"></a>Sequence-to-sequence基础</h1><p>基本的<code>seq2seq</code>模型包含了两个RNN，解码器和编码器，如下图所示：</p>
<p><img src="basic_seq2seq.png" alt="seq2seq模型"></p>
<p>每个矩形都表示着RNN的一个核，通常是GRU（Gated recurrent units）或者长短期记忆（LSTM）核。编码器和解码器可以使用相同的权重，或者，更常见的是，编码器和解码器分别使用不同的参数。多层神经网络已经成功地用于序列序列模型之中了。</p>
<p>在上面所说的模型之中，每个输入都会被编码成一个定长度的矢量,这也是唯一的传递入解码器的参数了。为了让解码器更直接的访问输入的参数，我们引入了一种行为，你可以<a href="http://arxiv.org/abs/1409.0473" target="_blank" rel="external">在这里</a>查到有关的信息。简而言之，在每个解码的阶段，这种方法可以让解码器查看输入的参数。一个多层的<code>seq2seq</code>的LSTM神经网络的处理行为就如图所示</p>
<p><img src="attention_seq2seq.png" alt="多层seq2seq模型"></p>
<h1 id="TensorFlow的seq2seq库"><a href="#TensorFlow的seq2seq库" class="headerlink" title="TensorFlow的seq2seq库"></a>TensorFlow的seq2seq库</h1><p>正如你所知道的，<code>seq2seq</code>有多种多样的形式，使用了不同的RNN核，但是万变不离其宗，其总是接受一个编码和解码的输入。TensorFlow也为此创建了一个模型：<code>tensorflow/tensorflow/python/ops/seq2seq.py</code>，最基本的RNN编码-解码器就像是这样子的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">outputs, states = basic_rnn_seq2seq(encoder_inputs, decoder_inputs, cell)</div></pre></td></tr></table></figure></p>
<p>在上述的<code>seq2seq</code>模型的参数之中，<code>encoder_inputs</code>是给予编码器数据的张量。比如在图中的A、B和C。相应的，<code>decoder_inputs</code>则是代表了给予解码器的输入。比如GO，W，X，Y和Z。</p>
<p><code>cell</code>这个参数则是表示的是<code>tf.contrib.rnn.RNNCell</code>的一个实例，也就是<code>seq2seq</code>使用的RNN核，你可以使用诸如<code>GRUCell</code>或者<code>LSTMCell</code>甚至是你自己定义的核。进而言之，<code>tf.contrib.rnn</code>提供了一系列的构建多层RNN的核的封装并且为输入输出添加一个<code>dropout</code>来防止过拟合。你可以在<a href="https://www.tensorflow.org/tutorials/recurrent" target="_blank" rel="external">RNN指导</a>看到如何这样做。</p>
<p><code>basic_rnn_seq2seq</code>的调用会返回两个参数<code>output</code>和<code>state</code>。他们都是和<code>decoder_inputs</code>同等长度的张量。望文生义，<code>output</code>代表着每层解码器的输出，比如图中的W，X，Y，Z，EOS，而<code>state</code>则代表的是每个时间步长解码器的内部状态。</p>
<p>在大量的<code>seq2seq</code>模型的应用之中，在t时输出的解码器的输出会被反馈，并且成为t+1时的编码器的输入。在测试的时候，当我们开始对一个序列进行解码的时候，这也是我们构建一个序列的表现。在训练的时候，通常我们都会同时提供正确的输入和输出，即使解码器之前输出的是一个错误的结果。在<code>seq2seq.py</code>之中的函数就支持<code>feed_previous</code>这个参数。举例而言，我们就可以使用嵌套的RNN模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">outputs, states = embedding_rnn_seq2seq(</div><div class="line">    encoder_inputs, decoder_inputs, cell,</div><div class="line">    num_encoder_symbols, num_decoder_symbols,</div><div class="line">    embedding_size, output_projection=<span class="keyword">None</span>,</div><div class="line">    feed_previous=<span class="keyword">False</span>)</div></pre></td></tr></table></figure>
<p>在<code>embedding_rnn_seq2seq</code>模型之中，所有的输入参数（包括<code>encoder_inputs</code>和<code>decoder_inputs</code>）都是整数的离散张量。他们会按照<code>word2vec</code>模型变成一个密集的表达。但是构建嵌套向量则需要离散值的最多的个数，分别是在编码端的<code>num_encoder_symbols</code>和解码端<code>decoder_inputs</code>两个参数。</p>
<p>在上面的代码之中，我们设置了<code>feed_previous</code>这个参数为<code>False</code>，这就意味着解码器会使用<code>decoder_inputs</code>这个张量。但是如果我们将其置为<code>True</code>，那么程序就只会使用第一个参数了，其他参数都会被无视，然后上一次的解码器输出将会被使用于下一次训练。这通常会被用于训练模型,让系统变得更加稳健。</p>
<p>另外一个很重要的参数就是<code>output_projection</code>了,嵌入模型的输出就会按照<code>num_decoder_symbols</code>（它们表示每个生成的符号的对数）大小的张量，当我们要训练一个样本集很大的模型的时候,直接存储这些张量变得不切实际了。这个时候返回一些小的张量才是明智之举，之后这些小张量就会被转换（project）成一个大的张量,这个时候就是<code>out_projection</code>的使用了。这就使得<code>seq2seq</code>能够使用softmax来确定loss了.这一点在<a href="http://arxiv.org/abs/1412.2007" target="_blank" rel="external">Jean et. al., 2014</a>里面说明的很清楚了</p>
<p>除了<code>basic_rnn_seq2seq</code>与<code>embedding_rnn_seq2seq</code>之外还有一些<code>seq2seq</code>的模型，其情况也是类似的，我们也不会很详细的描述了。</p>
<h1 id="基于神经网络的翻译模型"><a href="#基于神经网络的翻译模型" class="headerlink" title="基于神经网络的翻译模型"></a>基于神经网络的翻译模型</h1><p>核心的<code>seq2seq</code>在<code>tensorflow/tensorflow/python/ops/seq2seq.py</code>之中就被构建了，当然有一些很值得注意的小技巧，这些技巧在入门之中也是值得被应用的。</p>
<h2 id="softmax采样以及output-project"><a href="#softmax采样以及output-project" class="headerlink" title="softmax采样以及output_project"></a>softmax采样以及output_project</h2><p>所以正如上面所说的，我们将会使用<code>softmax</code>方法来处理大规模的词汇。<code>softmax</code>和<code>out_project</code>的实现将会在<code>seq2seq_model.py</code>看到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> num_samples &gt; <span class="number">0</span> <span class="keyword">and</span> num_samples &lt; self.target_vocab_size:</div><div class="line">  w_t = tf.get_variable(<span class="string">"proj_w"</span>, [self.target_vocab_size, size], dtype=dtype)</div><div class="line">  w = tf.transpose(w_t)</div><div class="line">  b = tf.get_variable(<span class="string">"proj_b"</span>, [self.target_vocab_size], dtype=dtype)</div><div class="line">  output_projection = (w, b)</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sampled_loss</span><span class="params">(labels, inputs)</span>:</span></div><div class="line">    labels = tf.reshape(labels, [<span class="number">-1</span>, <span class="number">1</span>])</div><div class="line">    <span class="comment"># We need to compute the sampled_softmax_loss using 32bit floats to</span></div><div class="line">    <span class="comment"># avoid numerical instabilities.</span></div><div class="line">    local_w_t = tf.cast(w_t, tf.float32)</div><div class="line">    local_b = tf.cast(b, tf.float32)</div><div class="line">    local_inputs = tf.cast(inputs, tf.float32)</div><div class="line">    <span class="keyword">return</span> tf.cast(</div><div class="line">        tf.nn.sampled_softmax_loss(</div><div class="line">            weights=local_w_t,</div><div class="line">            biases=local_b,</div><div class="line">            labels=labels,</div><div class="line">            inputs=local_inputs,</div><div class="line">            num_sampled=num_samples,</div><div class="line">            num_classes=self.target_vocab_size),</div><div class="line">        dtype)</div></pre></td></tr></table></figure>
<p>首先，请注意，如果样本数（默认为512）小于目标词汇大小，我们只构造一个采样<code>softmax</code>。对于小于512的词汇，只是使用标准的<code>softmax</code>损失可能是一个更好的主意。</p>
<p>正如你所见，我们构造了一个输出的工程，其包含了一个权重矩阵和偏差向量。如果<code>output_project</code>不为空，RNN核将会返回一个样本长度<code>size</code>的矢量而不是<code>target_vocab_size</code>,在此之后，正如<code>seq2seq_model.py</code>所示，你需要乘上权重矩阵并且加上偏差向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> output_projection <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">  <span class="keyword">for</span> b <span class="keyword">in</span> xrange(len(buckets)):</div><div class="line">    self.outputs[b] = [tf.matmul(output, output_projection[<span class="number">0</span>]) +</div><div class="line">                       output_projection[<span class="number">1</span>] <span class="keyword">for</span> ...]</div></pre></td></tr></table></figure>
<h1 id="bucking和填充"><a href="#bucking和填充" class="headerlink" title="bucking和填充"></a>bucking和填充</h1><p>除了采样的softmax方法外，我们的翻译工作也让我们开始使用桶（bucket）的方法，这也是一种对于变长度句子翻译的很好用的工具。当我们想从英文翻译到法语的时候，输入的英文的长度为L1而输出法语的长度则为L2。而我们现在已知英文从<code>encoder_input</code>进入法语从<code>decoder_input</code>输出（其标识有GO的前缀）,这样我们就需要一个<code>(L1,L2+1)</code>长的seq2seq模型，来对每一对英法文进行处理.这将导致一个庞大的图形，由许多非常相似的子图组成。另一方面，我们可以用特殊的<code>PAD</code>符号来填充每个句子。那么我们只需要一个seq2seq模型，用于填充长度。然而对于一些非常短的语句和词汇，我们的模型将会变得低效，编码和解码太多的<code>PAD</code>填充符会变得很没有意义。</p>
<p>似乎我们需要在对过短和过长句子的处理之间找到一个平衡点,我们会使用不同长度的桶，并且在桶上放置不同的句子并且填充他们至桶满。在<code>translate.py</code>之中，我们会使用以下的默认长度的桶。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">buckets = [(<span class="number">5</span>, <span class="number">10</span>), (<span class="number">10</span>, <span class="number">15</span>), (<span class="number">20</span>, <span class="number">25</span>), (<span class="number">40</span>, <span class="number">50</span>)]</div></pre></td></tr></table></figure>
<p>这意味着如果输入是具有3个令牌的英文句子，并且相应的输出是具有6个令牌的法语句子，那么它们将被放入第一个数据桶，并填充到编码器输入的长度为5，解码器输入的长度为10 。如果我们有一个8个令牌的英文句子，相应的法语句子有18个令牌，那么它们将不适用于（10,15）桶，所以（20,25）桶将被使用，即英文句子被填补到20个长度，而法国一到25个长度。</p>
<p>请记住，当构建解码器输入时，我们将特殊<code>GO</code>符号添加到到了里面。这是在<code>seq2seq_model.py</code>的<code>get_batch()</code>函数中完成的，其也会反转英语的输入。正如<a href="http://arxiv.org/abs/1409.3215" target="_blank" rel="external">Sutskever</a>所说，这有助于改善机器学习后的结果。现在有句英文<code>I go.</code>就会被分解为<code>[&quot;I&quot;, &quot;go&quot;, &quot;.&quot;]</code>，其将作为编码器的输入，而输出<code>Je vais.</code>则会被分解为<code>[&quot;Je&quot;, &quot;vais&quot;, &quot;.&quot;]</code>。其会被放入<code>(5,10)</code>的桶中。所以经过反转并且田冲后的输入就是<code>[PAD PAD &quot;.&quot; &quot;go&quot; &quot;I&quot;]</code>，而输出则是<code>[GO &quot;Je&quot; &quot;vais&quot; &quot;.&quot; EOS PAD PAD PAD PAD PAD]</code>。</p>
<h1 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h1><p>作者懒得翻译了，直接去Tensorflow网上找吧。。。<a href="https://www.tensorflow.org/tutorials/seq2seq#lets_run_it" target="_blank" rel="external">点我</a>前往</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://kidozh.github.io/2017/05/11/npuacm代码查重免注册使用/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="kido zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="kidozh">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/05/11/npuacm代码查重免注册使用/" itemprop="url">npuacm代码查重免注册使用</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-05-11T18:40:18+08:00">
                2017-05-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>最近备案成功，所以我也把NPUACM的实验站的代码查重的JPLAG版本以<strong>免</strong>注册的形式放到线上了，欢迎来耍～</p>
<p>地址：<a href="https://www.npuacm.info/plag/freejplagFile/" target="_blank" rel="external">https://www.npuacm.info/plag/freejplagFile/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://kidozh.github.io/2017/05/10/使用Tensorflow分析周杰伦的歌词/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="kido zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="kidozh">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/05/10/使用Tensorflow分析周杰伦的歌词/" itemprop="url">使用Tensorflow分析周杰伦的歌词</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-05-10T13:52:50+08:00">
                2017-05-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近一直在看TensorFlow的东西，也不知道怎么做才好，借着TF的word2vec的例子，我来分析一下周杰伦所唱的歌。</p>
<h1 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h1><p>首先，我准备了周杰伦唱过的歌的歌词，你可以在这里<a href="Jay_Chou.txt">周杰伦唱过的歌</a>下载这个例子。当然你有更好的例子的话，欢迎联系<a href="mailto:kidozh@kidozh.com" target="_blank" rel="external">本弱</a></p>
<p>根据TF官方的数据，其给的是一个英文的分词表，所以移花接木的，这里可以使用中文分词工具<code>jieba</code>来处理歌词的分词。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">words = []</div><div class="line"><span class="keyword">with</span> open(<span class="string">'Jay_Chou.txt'</span>,<span class="string">'r'</span>) <span class="keyword">as</span> f:</div><div class="line">    lyrics = f.read().split()</div><div class="line">    <span class="comment"># use jieba to cut the words</span></div><div class="line">    <span class="keyword">import</span> jieba</div><div class="line">    <span class="keyword">for</span> everySentence <span class="keyword">in</span> lyrics:</div><div class="line">        words.extend(jieba.cut(everySentence))</div></pre></td></tr></table></figure>
<p>这里的<code>words</code>是用于记录分词后的周杰伦的歌词的一个列表。</p>
<h1 id="创建一个字典来方便检索数据"><a href="#创建一个字典来方便检索数据" class="headerlink" title="创建一个字典来方便检索数据"></a>创建一个字典来方便检索数据</h1><p>这里根据TF的入门指导，选择字典来检索数据，并且为后面<code>skip-gram</code> 模型的生成提供指导。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> collections</div><div class="line">cnt = collections.Counter(words)</div><div class="line"><span class="comment"># print 15 most common</span></div><div class="line">print(cnt.most_common(<span class="number">15</span>),len(cnt.keys()))</div><div class="line"></div><div class="line"><span class="comment"># sample is 925</span></div><div class="line">vocabularySize = <span class="number">600</span></div><div class="line"></div><div class="line"><span class="comment"># build the dictionary and replace unknown figure with UNK</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildDataset</span><span class="params">(words, vocabularySize)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">    build the count dictionary for words</div><div class="line">    :param words: a list containing different words</div><div class="line">    :param vocabularySize: how many words will be choosen to analysis the lyrics</div><div class="line">    :return: data,count[(words,cnt),...],dict(words-&gt;cnt),reversedict(cnt-&gt;words)</div><div class="line">    '''</div><div class="line">    count = [[<span class="string">'UNK'</span>,<span class="number">-1</span>]]</div><div class="line">    <span class="comment"># choose 499 words</span></div><div class="line">    count.extend(collections.Counter(words).most_common(vocabularySize<span class="number">-1</span>))</div><div class="line">    dictionary = &#123;&#125;</div><div class="line">    <span class="comment"># place a rank according to `most common`</span></div><div class="line">    <span class="keyword">for</span> word,_ <span class="keyword">in</span> count:</div><div class="line">        dictionary[word] = len(dictionary)</div><div class="line"></div><div class="line">    data = []</div><div class="line">    unknownCnt = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment"># traverse all the words and get its rank corresponding to data</span></div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</div><div class="line">        <span class="comment"># if this words in most common 499, then label it</span></div><div class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> dictionary:</div><div class="line">            <span class="comment"># return its index in common dictionary</span></div><div class="line">            index = dictionary[word]</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># it belongs to unknown word</span></div><div class="line">            index = <span class="number">0</span></div><div class="line">            unknownCnt += <span class="number">1</span></div><div class="line">        <span class="comment"># set up a mapping between word and index</span></div><div class="line">        data.append(index)</div><div class="line">    <span class="comment"># update unknown number</span></div><div class="line">    count[<span class="number">0</span>][<span class="number">1</span>] = unknownCnt</div><div class="line">    reverseDict = dict(zip(dictionary.values(),dictionary.keys()))</div><div class="line">    <span class="keyword">return</span> data,count,dictionary,reverseDict</div><div class="line"></div><div class="line">data,countList,dictionary,reverseDict = buildDataset(words,vocabularySize)</div><div class="line"></div><div class="line"><span class="comment"># remove original data for saving memory</span></div><div class="line"><span class="keyword">del</span> words,lyrics</div><div class="line"></div><div class="line">print(<span class="string">'Most Common words'</span>,countList[:<span class="number">5</span>])</div><div class="line"><span class="comment"># get first letter</span></div><div class="line">print(<span class="string">'Sample data'</span>, data[:<span class="number">10</span>], [reverseDict[i] <span class="keyword">for</span> i <span class="keyword">in</span> data[:<span class="number">10</span>]])</div></pre></td></tr></table></figure>
<p>这里使用<code>collection</code>这个高效的容器来检索前600个出现次数最多的词，并且检索出来。然后将检索的前600个字编号。</p>
<p>创建一个索引数组，遍历原始数据中的列表，如果当前遍历对象是600个字中的一员，那么对应索引数组中的成员就赋予600个字的标号，如果不是，程序就会认为其是不可识别的，将其指向未知这个字符（UNK）的标号（应该是0），遍历结束后，补充上更新后的不可识别的字符。这样数据就转换成了一个带有索引编号的数组，一个字符计数的字典，一个标号的字典以及其反式字典。</p>
<p>然后删除掉原始数据，以便节省内存空间。</p>
<h1 id="建立skip-gram模型"><a href="#建立skip-gram模型" class="headerlink" title="建立skip-gram模型"></a>建立skip-gram模型</h1><h2 id="什么是skip-gram模型？"><a href="#什么是skip-gram模型？" class="headerlink" title="什么是skip-gram模型？"></a>什么是skip-gram模型？</h2><p>和词袋模型相反，<code>skip-gram</code>是通过从一个文字来预测的上下文。</p>
<p>其实, 用一个向量唯一标识一个word已经提出有一段时间了. Tomáš Mikolov的word2vec算法的一个不同之处在于, 他把一个word映射到高维(50到300维), 并且在这个维度上有了很多有意思的语言学特性, 比如单词”Rome”的表达<code>vec(‘Rome’)</code>, 可以是<code>vec(‘Paris’) – vec(‘France’) + vec(‘Italy’)</code>的计算结果.</p>
<blockquote>
<p>向量空间模型 (VSMs)将词汇表达（嵌套）于一个连续的向量空间中，语义近似的词汇被映射为相邻的数据点。向量空间模型在自然语言处理领域中有着漫长且丰富的历史，不过几乎所有利用这一模型的方法都依赖于 分布式假设，其核心思想为出现于上下文情景中的词汇都有相类似的语义。采用这一假设的研究方法大致分为以下两类：基于技术的方法 (e.g. 潜在语义分析)， 和 预测方法 (e.g. 神经概率化语言模型).</p>
</blockquote>
<h2 id="如何建立一个skip-gram模型？"><a href="#如何建立一个skip-gram模型？" class="headerlink" title="如何建立一个skip-gram模型？"></a>如何建立一个skip-gram模型？</h2><blockquote>
<p>建议直接参考<a href="http://www.tensorfly.cn/tfdoc/tutorials/word2vec.html" target="_blank" rel="external">TensorFlow中文教程</a></p>
</blockquote>
<p><code>the quick brown fox jumped over the lazy dog</code></p>
<p>我们首先对一些单词以及它们的上下文环境建立一个数据集。我们可以以任何合理的方式定义‘上下文’，而通常上这个方式是根据文字的句法语境的（使用语法原理的方式处理当前目标单词可以看一下这篇文献 <a href="https://levyomer.files.wordpress.com/2014/04/dependency-based-word-embeddings-acl-2014.pdf" target="_blank" rel="external">Levy et al</a>.，比如说把目标单词左边的内容当做一个‘上下文’，或者以目标单词右边的内容，等等。现在我们把目标单词的左右单词视作一个上下文， 使用大小为1的窗口，这样就得到这样一个由<code>(上下文, 目标单词)</code> 组成的数据集：</p>
<p><code>([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...</code></p>
<p>前文提到Skip-Gram模型是把目标单词和上下文颠倒过来，所以在这个问题中，举个例子，就是用<code>&#39;quick&#39;</code>来预测 <code>&#39;the&#39;</code> 和 <code>&#39;brown&#39;</code> ，用 <code>&#39;brown&#39;</code> 预测 <code>&#39;quick&#39;</code> 和 <code>&#39;brown&#39;</code> 。因此这个数据集就变成由<code>(输入, 输出)</code>组成的：</p>
<p><code>(quick, the), (quick, brown), (brown, quick), (brown, fox), ...</code></p>
<p>目标函数通常是对整个数据集建立的，但是本问题中要对每一个样本（或者是一个<code>batch_size</code> 很小的样本集，通常设置为<code>16 &lt;= batch_size &lt;= 512</code>）在同一时间执行特别的操作，称之为随机梯度下降 <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank" rel="external">SGD</a>。我们来看一下训练过程中每一步的执行。</p>
<p>假设用 t 表示上面这个例子中quick 来预测 the 的训练的单个循环。用 <code>num_noise</code> 定义从噪声分布中挑选出来的噪声（相反的）单词的个数，通常使用一元分布，P(w)。为了简单起见，我们就定<code>num_noise=1</code>，用<code>sheep</code>选作噪声词。接下来就可以计算每一对观察值和噪声值的损失函数了。</p>
<h2 id="构建针对skip-gram的数据集"><a href="#构建针对skip-gram的数据集" class="headerlink" title="构建针对skip-gram的数据集"></a>构建针对skip-gram的数据集</h2><p><code>skip-gram</code>模型可以理解为每次从一个长度为<code>[skipWindow...,target,skipWindow... ]</code>的样本中找出位于两端的元素和目标输出构成的一个<code>(输出,输入)</code>的一个样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line">dataIndex = <span class="number">0</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line"><span class="comment"># generate a training batch for skip-gram model</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateSkipGramBatch</span><span class="params">(batchSize,numSkip,skipWinnow)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">    generate a training batch for skip-gram model</div><div class="line">    :param batchSize: the size of batch</div><div class="line">    :param numSkip: How many times to reuse an input to generate a label.</div><div class="line">    :param skipWinnow: How many words to consider left and right.</div><div class="line">    :return: </div><div class="line">    '''</div><div class="line">    <span class="keyword">global</span> dataIndex</div><div class="line">    <span class="keyword">assert</span> batchSize % numSkip == <span class="number">0</span></div><div class="line">    <span class="keyword">assert</span> numSkip &lt;= <span class="number">2</span>* skipWinnow</div><div class="line"></div><div class="line">    batch = np.ndarray(shape=(batchSize),dtype=np.int32)</div><div class="line">    labels = np.ndarray(shape=(batchSize,<span class="number">1</span>),dtype=np.int32)</div><div class="line"></div><div class="line">    span = <span class="number">2</span> * skipWinnow + <span class="number">1</span> <span class="comment"># [ skip_window target skip_window ]</span></div><div class="line">    <span class="comment"># a binary queue to filter the words</span></div><div class="line">    buffer = collections.deque(maxlen=span)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(span):</div><div class="line">        <span class="comment"># construct a winnow in data</span></div><div class="line">        buffer.append(data[dataIndex])</div><div class="line">        dataIndex = (dataIndex + <span class="number">1</span> ) % len(data)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batchSize // numSkip):</div><div class="line">        <span class="comment"># ensure target located in the center of window</span></div><div class="line">        target = skipWinnow</div><div class="line">        aviodTarget = [skipWinnow]</div><div class="line"></div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(numSkip):</div><div class="line">            <span class="keyword">while</span> target <span class="keyword">in</span> aviodTarget:</div><div class="line">                <span class="comment"># get a number that is not located in the center of window</span></div><div class="line">                target = random.randint(<span class="number">0</span>,span<span class="number">-1</span>)</div><div class="line">            aviodTarget.append(target)</div><div class="line">            <span class="comment"># pick up</span></div><div class="line">            batch[i * numSkip + j ] = buffer[skipWinnow]</div><div class="line">            labels[i * numSkip + j,<span class="number">0</span>] = buffer[target]</div><div class="line"></div><div class="line">        <span class="comment"># join the next words</span></div><div class="line">        buffer.append(data[dataIndex])</div><div class="line">        dataIndex = (dataIndex + <span class="number">1</span>) % len(data)</div><div class="line"></div><div class="line">    dataIndex = (dataIndex + len(data) - span) % len(data)</div><div class="line">    <span class="keyword">return</span> batch,labels</div></pre></td></tr></table></figure>
<p>在这个函数之中，<code>batchSize</code>指的是每次选择的样本的数量，而<code>numSkip</code>指的则是每次target和周围词的复用率，<code>dataIndex</code>作为一个全局变量则是起到一个类似于指针的作用。</p>
<p>代码在初始化的时候，初始化了定形状的<code>batch</code>和<code>label</code>矩阵，接着，代码将会从当前<code>dataIndex</code>取<code>2 * skipWinnow + 1</code>个长度为<code>2 * skipWinnow + 1</code>压进一个长度为双端队列。接着根据指示其就会选出target周围的<code>numSkip</code>个词，接着索引指向下一个字符，继续操作。直至选出<code>batchSize</code>个样本出来。<code>batch</code>是选出的输出词，而<code>label</code>则是输入词。</p>
<h1 id="开始skip-gram的训练"><a href="#开始skip-gram的训练" class="headerlink" title="开始skip-gram的训练"></a>开始skip-gram的训练</h1><p>预设定条件之后，就可以开始<code>skip-gram</code>的训练了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line">batchSize = <span class="number">128</span></div><div class="line">embeddingSize = <span class="number">128</span> <span class="comment"># dimension of emblemding vector</span></div><div class="line">skipWindow = <span class="number">1</span> <span class="comment"># how many words contains in both left and right direction</span></div><div class="line">numSkip = <span class="number">2</span> <span class="comment"># how many times words will be used in sampling</span></div><div class="line"></div><div class="line"><span class="comment"># We pick a random validation set to sample nearest neighbors. Here we limit the</span></div><div class="line"><span class="comment"># validation samples to the words that have a low numeric ID, which by</span></div><div class="line"><span class="comment"># construction are also the most frequent.</span></div><div class="line"></div><div class="line">validSize = <span class="number">16</span> <span class="comment"># Random set of words to evaluate similarity on.</span></div><div class="line">validWindow = <span class="number">100</span> <span class="comment"># Only pick dev samples in the head of the distribution.</span></div><div class="line">validExample = np.random.choice(validWindow,validSize,replace=<span class="keyword">False</span>)</div><div class="line">numSample = <span class="number">64</span> <span class="comment"># number of negative sample</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> math</div><div class="line"></div><div class="line">graph = tf.Graph()</div><div class="line"></div><div class="line"><span class="keyword">with</span> graph.as_default():</div><div class="line">    <span class="comment"># Input data.</span></div><div class="line">    train_inputs = tf.placeholder(tf.int32, shape=[batchSize])</div><div class="line">    train_labels = tf.placeholder(tf.int32, shape=[batchSize, <span class="number">1</span>])</div><div class="line">    valid_dataset = tf.constant(validExample, dtype=tf.int32)</div><div class="line">    </div><div class="line">    <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</div><div class="line">        <span class="comment"># Look up embeddings for inputs.</span></div><div class="line">        embeddings = tf.Variable(</div><div class="line">            tf.random_uniform([vocabularySize, embeddingSize], <span class="number">-1.0</span>, <span class="number">1.0</span>))</div><div class="line">        embed = tf.nn.embedding_lookup(embeddings, train_inputs)</div><div class="line"></div><div class="line">        <span class="comment"># Construct the variables for the NCE loss</span></div><div class="line">        nce_weights = tf.Variable(</div><div class="line">            tf.truncated_normal([vocabularySize, embeddingSize],</div><div class="line">                                stddev=<span class="number">1.0</span> / math.sqrt(embeddingSize)))</div><div class="line">        nce_biases = tf.Variable(tf.zeros([vocabularySize]))</div><div class="line"></div><div class="line">        <span class="comment"># Compute the average NCE loss for the batch.</span></div><div class="line">        <span class="comment"># tf.nce_loss automatically draws a new sample of the negative labels each</span></div><div class="line">        <span class="comment"># time we evaluate the loss.</span></div><div class="line">        loss = tf.reduce_mean(</div><div class="line">            tf.nn.nce_loss(weights=nce_weights,</div><div class="line">                           biases=nce_biases,</div><div class="line">                           labels=train_labels,</div><div class="line">                           inputs=embed,</div><div class="line">                           num_sampled=numSample,</div><div class="line">                           num_classes=vocabularySize))</div><div class="line"></div><div class="line">        <span class="comment"># Construct the SGD optimizer using a learning rate of 1.0.</span></div><div class="line">        optimizer = tf.train.GradientDescentOptimizer(<span class="number">1.0</span>).minimize(loss)</div><div class="line"></div><div class="line">        <span class="comment"># Compute the cosine similarity between minibatch examples and all embeddings.</span></div><div class="line">        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), <span class="number">1</span>, keep_dims=<span class="keyword">True</span>))</div><div class="line">        normalized_embeddings = embeddings / norm</div><div class="line">        valid_embeddings = tf.nn.embedding_lookup(</div><div class="line">            normalized_embeddings, valid_dataset)</div><div class="line">        similarity = tf.matmul(</div><div class="line">            valid_embeddings, normalized_embeddings, transpose_b=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Add variable initializer.</span></div><div class="line">        init = tf.global_variables_initializer()</div></pre></td></tr></table></figure>
<p>首先使用了两个占位符，其主要作为<code>batch</code>和<code>label</code>所feed的对象。由于我们没有GPU配置（渣电脑是AMD的显卡，哈哈哈）。然后建立了一个形状为<code>[vocabularySize, embeddingSize]</code>的一个嵌套的矩阵。我们用唯一的随机值来初始化这个大矩阵。</p>
<p>然后我们需要对批数据中的单词建立嵌套向量，TensorFlow提供了方便的工具函数。也就是<code>tf.nn.embedding_lookup</code></p>
<p>对噪声-比对的损失计算就使用一个逻辑回归模型。对此，我们需要对语料库中的每个单词定义一个权重值（weight）和偏差值（baise）。(也可称之为输出权重 与之对应的 输入嵌套值)。</p>
<img src="/2017/05/10/使用Tensorflow分析周杰伦的歌词/word2vec_NN.png" alt="word2vec实例" title="word2vec实例">
<p>所以说word2vec是只有一个隐层的全连接神经网络, 用来预测给定单词的关联度大的单词.WI 的大小是VxN, V是单词字典的大小, 每次输入是一个单词, N是你设定的隐层大小.</p>
<p>对于整个数据集，当梯度下降的过程中不断地更新参数，对应产生的效果就是不断地移动每个单词的嵌套向量，直到可以把真实单词和噪声单词很好得区分开。所以这里使用的是<code>NCE</code>函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">loss = tf.reduce_mean(</div><div class="line">    tf.nn.nce_loss(weights=nce_weights,</div><div class="line">                   biases=nce_biases,</div><div class="line">                   labels=train_labels,</div><div class="line">                   inputs=embed,</div><div class="line">                   num_sampled=numSample,</div><div class="line">                   num_classes=vocabularySize))</div></pre></td></tr></table></figure>
<h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><p>训练的过程很简单，只要在循环中使用<code>feed_dict</code>不断给占位符填充数据，同时调用<code>session.run</code>即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Step 5: Begin training.</span></div><div class="line">num_steps = <span class="number">100001</span></div><div class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> xrange</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</div><div class="line">    <span class="comment"># We must initialize all variables before we use them.</span></div><div class="line">    init.run()</div><div class="line">    print(<span class="string">"Initialized"</span>)</div><div class="line"></div><div class="line">    average_loss = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> xrange(num_steps):</div><div class="line">        batch_inputs, batch_labels = generateSkipGramBatch(</div><div class="line">            batchSize, numSkip, skipWindow)</div><div class="line">        feed_dict = &#123;train_inputs: batch_inputs, train_labels: batch_labels&#125;</div><div class="line"></div><div class="line">        <span class="comment"># We perform one update step by evaluating the optimizer op (including it</span></div><div class="line">        <span class="comment"># in the list of returned values for session.run()</span></div><div class="line">        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)</div><div class="line">        average_loss += loss_val</div><div class="line"></div><div class="line">        <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</div><div class="line">            <span class="keyword">if</span> step &gt; <span class="number">0</span>:</div><div class="line">                average_loss /= <span class="number">2000</span></div><div class="line">            <span class="comment"># The average loss is an estimate of the loss over the last 2000 batches.</span></div><div class="line">            print(<span class="string">"Average loss at step "</span>, step, <span class="string">": "</span>, average_loss)</div><div class="line">            average_loss = <span class="number">0</span></div><div class="line"></div><div class="line">        <span class="comment"># Note that this is expensive (~20% slowdown if computed every 500 steps)</span></div><div class="line">        <span class="keyword">if</span> step % <span class="number">10000</span> == <span class="number">0</span>:</div><div class="line">            sim = similarity.eval()</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> xrange(validSize):</div><div class="line">                valid_word = reverseDict[validExample[i]]</div><div class="line">                top_k = <span class="number">8</span>  <span class="comment"># number of nearest neighbors</span></div><div class="line">                nearest = (-sim[i, :]).argsort()[<span class="number">1</span>:top_k + <span class="number">1</span>]</div><div class="line">                log_str = <span class="string">"Nearest to %s:"</span> % valid_word</div><div class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> xrange(top_k):</div><div class="line">                    close_word = reverseDict[nearest[k]]</div><div class="line">                    log_str = <span class="string">"%s %s,"</span> % (log_str, close_word)</div><div class="line">                print(log_str)</div><div class="line">    final_embeddings = normalized_embeddings.eval()</div><div class="line"></div><div class="line"><span class="comment"># Step 6: Visualize the embeddings.</span></div><div class="line"><span class="keyword">import</span> matplotlib</div><div class="line"></div><div class="line">matplotlib.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'PingFang SC'</span>]</div><div class="line">matplotlib.rcParams[<span class="string">'font.family'</span>]=<span class="string">'sans-serif'</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_with_labels</span><span class="params">(low_dim_embs, labels, filename=<span class="string">'tsne.svg'</span>)</span>:</span></div><div class="line">    <span class="keyword">assert</span> low_dim_embs.shape[<span class="number">0</span>] &gt;= len(labels), <span class="string">"More labels than embeddings"</span></div><div class="line">    plt.figure(figsize=(<span class="number">18</span>, <span class="number">18</span>))  <span class="comment"># in inches</span></div><div class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels):</div><div class="line">        x, y = low_dim_embs[i, :]</div><div class="line">        plt.scatter(x, y)</div><div class="line">        plt.annotate(label,</div><div class="line">                     xy=(x, y),</div><div class="line">                     xytext=(<span class="number">5</span>, <span class="number">2</span>),</div><div class="line">                     textcoords=<span class="string">'offset points'</span>,</div><div class="line">                     ha=<span class="string">'right'</span>,</div><div class="line">                     va=<span class="string">'bottom'</span>)</div><div class="line"></div><div class="line">    plt.savefig(filename)</div><div class="line"></div><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</div><div class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">    tsne = TSNE(perplexity=<span class="number">30</span>, n_components=<span class="number">2</span>, init=<span class="string">'pca'</span>, n_iter=<span class="number">5000</span>)</div><div class="line">    plot_only = <span class="number">500</span></div><div class="line">    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])</div><div class="line">    labels = [reverseDict[i] <span class="keyword">for</span> i <span class="keyword">in</span> xrange(plot_only)]</div><div class="line">    plot_with_labels(low_dim_embs, labels)</div><div class="line"></div><div class="line"><span class="keyword">except</span> ImportError:</div><div class="line">    print(<span class="string">"Please install sklearn, matplotlib, and scipy to visualize embeddings."</span>)</div></pre></td></tr></table></figure>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>还是很尴尬的。。。</p>
<p><img src="tsne.svg" alt="下载观看"></p>
<h1 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h1><p>可以利用这个为之后的<code>seq2seq</code>模型的建立提供支持</p>
<h1 id="所有的代码下载"><a href="#所有的代码下载" class="headerlink" title="所有的代码下载"></a>所有的代码下载</h1><p>点击<a href="web2vec.py">这里</a></p>
<p><strong>Note</strong> : 可能有乱码哦～</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://kidozh.github.io/2017/04/29/transfer-blog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="kido zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="kidozh">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/04/29/transfer-blog/" itemprop="url">博客转移到github.io了</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-29T20:06:58+08:00">
                2017-04-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>由于PHP的界面实在是满足不了我的需求，所以现在我把我的博客导出后放到了github page。</p>
<p>欢迎来访啊～～～</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://kidozh.github.io/2017/04/05/e7-94-a8git-e9-83-a8-e7-bd-b2-e7-bd-91-e7-ab-99-e4-bb-a3-e7-a0-81-e5-88-b0-e7-94-9f-e4-ba-a7-e7-8e-af-e5-a2-83vps/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="kido zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="kidozh">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/04/05/e7-94-a8git-e9-83-a8-e7-bd-b2-e7-bd-91-e7-ab-99-e4-bb-a3-e7-a0-81-e5-88-b0-e7-94-9f-e4-ba-a7-e7-8e-af-e5-a2-83vps/" itemprop="url">用git部署网站代码到生产环境VPS</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-05T19:28:27+08:00">
                2017-04-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/未分类/" itemprop="url" rel="index">
                    <span itemprop="name">未分类</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>之前开发网页的时候，经常要把代码传到自己的VPS上，平常用FTP传整个代码很慢，所以就像到了用Git push到VPS上。</p>
<h1 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> /var</div><div class="line">mkdir git &amp;&amp; <span class="built_in">cd</span> git</div><div class="line">mkdir your_site.git &amp;&amp; <span class="built_in">cd</span> your_site.git</div><div class="line">git init --bare</div></pre></td></tr></table></figure>
<p><code>--bare</code>的意思是，该文件夹是我们的代码仓库，它将不会放源代码而只是做版本控制。</p>
<h2 id="Hook钩子"><a href="#Hook钩子" class="headerlink" title="Hook钩子"></a>Hook钩子</h2><p>我们将会使用<code>post-receive</code>钩子</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ls</div></pre></td></tr></table></figure>
<p>你可以看到有hooks文件夹已经为我们创建好了,而且里面也有各种钩子的样例</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> hooks</div></pre></td></tr></table></figure>
<p>创建我们自己的<code>post-receive</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vim post-receive</div></pre></td></tr></table></figure>
<p>输入下面的命令到这个文件之中：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/sh</span></div><div class="line">git --work-tree=生产环境网站文件夹位置 --git-dir=/var/git/your_site.git checkout <span class="_">-f</span></div></pre></td></tr></table></figure>
<p><code>git-dir</code>指的是仓库的地址， <code>work-tree</code>则是存放代码的位置，也就是我们的网站的源代码的位置。 接下来则是要保证它可以运行：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">chmod +x post-receive</div></pre></td></tr></table></figure>
<h2 id="本地"><a href="#本地" class="headerlink" title="本地"></a>本地</h2><p>一般情况是你已经有了自己的git项目了，那么只需要添加vps的仓库地址就行了</p>
<pre><code>git remote add myVPS-sitename ssh://user@mydomain.com/var/git/your_site.git`
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">&apos;myVPS-sitename&apos;只是这个远程连接的名称，你可以同时有多个远程连接，每次push的时候指定名称即可将代码上传到不同的仓库。</div><div class="line"></div><div class="line">如果你本地还没有项目代码：</div></pre></td></tr></table></figure>
<p><code>cd 项目地址
    git init</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">添加一个README.ME文件后</div></pre></td></tr></table></figure></p>
<p><code>git add .
    git commit -m &quot;项目初始&quot;</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">接下来我们便可以将代码push到vps了：</div></pre></td></tr></table></figure></p>
<p>`git push myVPS-sitename master</p>
<p><code>master</code>指定的是master分支，如果你有其他分支也可以push其他分支。.</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这只是最基本的设置，利用hook结合一些自己编写的脚本我们还可以做很多事情。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://kidozh.github.io/2017/01/28/e5-9f-ba-e4-ba-8ek-gram-e7-9a-84winnowing-e7-89-b9-e5-be-81-e6-8f-90-e5-8f-96-e5-89-bd-e7-aa-83-e6-9f-a5-e9-87-8d-e6-a3-80-e6-b5-8b-e6-8a-80-e6-9c-af/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="kido zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="kidozh">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/01/28/e5-9f-ba-e4-ba-8ek-gram-e7-9a-84winnowing-e7-89-b9-e5-be-81-e6-8f-90-e5-8f-96-e5-89-bd-e7-aa-83-e6-9f-a5-e9-87-8d-e6-a3-80-e6-b5-8b-e6-8a-80-e6-9c-af/" itemprop="url">基于K-gram的winnowing特征提取剽窃查重检测技术</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-01-28T07:10:23+08:00">
                2017-01-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这篇文章主要来自于<a href="http://zkread.com/article/1130090.html" target="_blank" rel="external">http://zkread.com/article/1130090.html</a>，其参考的论文是MOSS的查重原理：<a href="http://theory.stanford.edu/~aiken/publications/papers/sigmod03.pdf" target="_blank" rel="external">http://theory.stanford.edu/~aiken/publications/papers/sigmod03.pdf</a>。</p>
<h1 id="winnowing算法"><a href="#winnowing算法" class="headerlink" title="winnowing算法"></a>winnowing算法</h1><p>winnowing也就是筛除的意思，也就是说，我们需要从源代码之中直接删除掉无用的信息，提取出特征值来匹配。</p>
<h1 id="k-gram模型"><a href="#k-gram模型" class="headerlink" title="k-gram模型"></a>k-gram模型</h1><p>K-gram/ n-gram 英文翻译过来就叫做n元语法模型 参见维基百科： <a href="https://en.wikipedia.org/wiki/N-gram" target="_blank" rel="external">n-gram English version</a> <a href="https://zh.wikipedia.org/wiki/N%E5%85%83%E8%AF%AD%E6%B3%95" target="_blank" rel="external">n-gram 中文版</a></p>
<p>k-gram 就是将一个连续的文本进行切割，每一个部分的长度都是k。当长度为1，2, 3时 分别对应的名称叫做 一元语法（1-gram unigram）, 二元语法（bigram）,三元语法（trigram）.</p>
<p>为了形象的说明这个问题，我们来举一个简单的例子。有一个简单的文档 叫做A，由字母yabbadabbadoo组成：</p>
<p>这个时候我们在A这个文档上取一个大小为3的滑动窗口，就得到了一个3-gram 的集合：</p>
<p><img src="/wp-content/uploads/2017/01/k-grams.png" alt=""></p>
<p>A : yab abb bba dad ada dab abb bba bad ado doo</p>
<p>我们把这个集合中的元素都称为shingle.</p>
<p>下面我们再来看另一个文档C，文档C由下面的这些字符组成的：doobeedoobeedoo：</p>
<p>我们在为3构建一个3-gram 的集合，那么组成的shingle就是下面这个样子的：</p>
<p>C : doo oob obe bee eed edo doo oob obe bee eed edo doo</p>
<p>那么我们要比较A和C之间的相似性，我们很容易看出来我们只要比较他们的Shingle集合中相似的有多少就可以了。 在A和C集合中我们可以发现他们共有的元素是doo,但是他在A集合中出现了一次，在C集合中出现了三次。</p>
<p>那么这里就有两种可能，一种就是这是偶然的，还有一种假设就是万一真的存在剽窃呢，那就是说明doo 从A中被复制到C中三次。</p>
<p>但是其实这种情况，完全是取决与我们的K</p>
<p>看下面我们对A和C进行重新的划分，我们去k=4</p>
<p>A : yabb abba bbad bada adab dabb abba bbad dado adoo C : doob oobe obee beed eedo edoo doob oobe obee beed eedo edoo</p>
<p>可以看出现在这样子划分，这两个字符串是完全没有相似性可言的。</p>
<p><strong>注意：</strong> 通过我们前面的观察，或者计算，对于一个含有N个字符的文档，我们按照k来划分得到的shingle 的个数是： N-K+1</p>
<h2 id="K的选取"><a href="#K的选取" class="headerlink" title="K的选取"></a>K的选取</h2><p>根据上面的分析我们知道，在这里最重要的就是选取K值。那么我们要怎么分割呢，一般我们文档的常见单词（不重要的出现频率大）的长度必须要<strong><em>小于</em></strong> K.</p>
<p>注意注意重点来了！！！</p>
<p>我们选取的K必须大于文档中常见的不重要的单词，比如说在一个文档里，中文文档，最常见的单词应该是“的”，这个次几乎是每个文档都会出现的，但是这个次可能对于文档的相似性判断没有任何重要性而言。</p>
<p>又比如说我们英文当中的the 和我们程序当中的if 都是我们不太关心的，所以K的取值一般要比这些单词要大。这样就很好的剔除了文档中自然产生的相似性。</p>
<p>我们使用论文中的原话来讲，我们要查找一些我们感兴趣的信息，重要的shingle的长度必须要大于K，这些使我们感兴趣的，比K小的就是我们不敢兴趣的。你看，这是不是就是一个简单的过滤过程，首先通过K我们可以过滤掉一些信息。</p>
<p>k-gram 最有意思的特征就是，在某种程序上他对于排序是不敏感的。注意关键词， <strong>某种程度</strong>上是不敏感的. 这样可以防止有的人重新排列了我们的文档，这种情况在代码中最为常见，比如说一个代码中有两个类，我们先后调换一下位置，这两份文档还是不能躲过我们k-gram hash 算法的火眼金睛。</p>
<p>比如说我们将 yabbadabbadoo 重新排列成 bbadooyabbada 混淆之后我们得到文档的shingle集合是： A： bba bad ado doo ooy oya yab abb bba bad ada</p>
<p>通过观察我们可以发现，3连字任然完整无缺的出现在其中，那么我们把这个文档和我们的C文档比较一下，他们的相似度是不变的。</p>
<h1 id="hash算法"><a href="#hash算法" class="headerlink" title="hash算法"></a>hash算法</h1><p>但是假设把这些信息都纳入计算，那么计算的开销无疑是巨大的。所以我们选择hash来直接提取特征值。</p>
<p>Hash 算法就是把任意长度的输入，通过散列算法，变换成固定长度的输出。该输出就是散列值。这种转换是一种压缩映射，散列值的空间一般远小于输入的空间。 但是如果不同的数据通过hash 算法得到了相同的输出，这个就叫做碰撞，因此不可能从散列值来唯一确定输入值。</p>
<p>一般的hash 算法我们都要求满足几个条件：</p>
<ul>
<li><p>单向性（one-way）, 从预映射，能够简单迅速的得到散列值，而在计算上不可能构造一个预映射，使其散列结果等于某个特定的散列值，即构造相应的M=H-1(h)不可行。这样，散列值就能在统计上唯一的表征输入值，因此，密码学上的 Hash 又被称为”消息摘要(messagedigest)”，就是要求能方便的将”消息”进行”摘要”，但在”摘要”中无法得到比”摘要”本身更多的关于”消息”的信息。</p>
</li>
<li><p>第二是抗冲突性(collision-resistant)，即在统计上无法产生2个散列值相同的预映射。给定M，计算上 无法找到M’，满足H(M)=H(M’) ，此谓弱抗冲突性；计算上也难以寻找一对任意的M和M’，使满足H(M)=H(M’) ，此谓强抗冲突性。要求”强抗冲突性”主要是为了防范 所谓”生日攻击(birthdayattack)”，在一个10人的团 体中，你能找到和你生日相同的人的概率是2.4%，而在同一团体中，有2人生日相同的概率是11.7%。类似的， 当预映射的空间很大的情况下，算法必须有足够的强度来保证不能轻易找到”相同生日”的人。</p>
</li>
<li><p>第三是映射分布均匀性和差分分布均匀性，散列结果中，为 0 的 bit 和为 1 的 bit ，其总数应该大致 相等；输入中一个 bit的变化，散列结果中将有一半以上的 bit 改变，这又叫做”雪崩效应(avalanche effect)”； 要实现使散列结果中出现 1bit的变化，则输入中至少有一半以上的 bit 必须发生变化。其实质是必须使输入 中每一个 bit 的信息， 尽量均匀的反映到输出的每一个 bit上去；输出中的每一个 bit，都是输入中尽可能 多 bit 的信息一起作用的结果。</p>
</li>
</ul>
<p>hash 算法最常用的就是加减乘除和移位运算。我们先来看几个常用常见的哈希函数吧。</p>
<p>观察上面的这些hash算法我们可以发现，输入都是一些字符串，我们需要对字符串进行操作，并且是对字符串的<strong>每一个位置上</strong>的字符串进行操作，移位加减乘除，等运算然后得到我们的散列置。在这里我们采用的hash 算法是下面这个：</p>
<p>[ c<em>{1}*b^{k-1}+c</em>{2}<em>b^{k-2}+….+c_{k-1}</em>b+c_{k} ]</p>
<p>其中H表示的是映射关系，这里操作的对象是我们的每一个shingle，所以C1…CK表示的是一个有K位的元model ，将每一个C按照我们的公式进行计算得到一个hash 值，这里的b 表示的是一个基底（Base） 这里是用户自己设定的某个值，我们一般选取一个质数来做我们的基底，按照上面的公式我们计算得到我们每个 Shingle的hash 值。</p>
<p>有了我们的hash算法，我们就可以计算每一个shingle的hash值。 我们的文档有N，按照k 来划分，得到的shingle 总共是N-K+1个。所以我们要计算N-K+1个长度为K的shingle的hash.</p>
<p>其实计算量还是很大的，不要着急，我们后面会讲怎么改进的，现在先卖个关子。</p>
<h1 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h1><p>什么是特征？ 应该是能很快区分出你是谁的点，我们把这些你独一无二的东西叫做我们的“特”征。特征一定是能快速标记你是谁的东西。 在软件文本的检测中这个道理同样适用。</p>
<p>一个软件，它总有一些比较有价值的代码，和一些大众代码，我们谁都可以实现的。你有我有大家有的东西，显然不可能做特征。</p>
<p>我们在破案的时候，常常采用指纹，DNA，掌纹，牙齿的结构，等等来确定这个人是谁。我们没有必要比较所有的特征，因为那是没有任何效率的。比个DNA就能搞定的事情，你非要把身上的每一块肥肉都拿来判别一下，这个行为就是来搞笑的。</p>
<p>所以我们没有必要去比较一个文档的所有的shingle的hash值，我们只需要比较一些特定的hash 值，就可以了。这个时候就涉及到特征的提取和选择，到底那些特征是该保留的，那些特征是该舍弃的？</p>
<p>现在我们有一整个文档的hash 值，那有的人就说了，那我们每隔几个选一个hash 值带代替整个文档，让这些哈希值作为我们整个文档的“胎记”birthmark 或者是”指纹”fingerprint 也就是我们的特征。</p>
<p>OK，这个想法却是减小了我们的特征值，但是有效么？</p>
<p>我们思考一下.</p>
<p>随机每隔相通的距离选取一个hash值，这样的做法有什么弊端么？有可能这些哈希值全部都是不重要的信息的hash, 你用这些值根本找不到剽窃样本。</p>
<p>所以某种程度上效果不好。</p>
<p>之后又有人 提出了我们使用 θmodp = 0 的方法来选择我们的hash 值。这样又使得我们的选择更加的随机。</p>
<p>假设我们有一组hash 值，是这个样子的：</p>
<p>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 77 72 42 17 98 50 17 98 8 88 67 39 77 72 42 17 98</p>
<p>那我们选取 mod 4 =0 的哈希那我们就得到了 四个hash值做我们文档的特征值</p>
<p>我们得到的特征值就是： （1:72）， （ 8:8）， （9:88） ， （13：72）</p>
<p>这个算法比上面的选取固定值又提升了一部，使得选取更加的随机化。但是，有问题么？？</p>
<p>假设我们的散列值是下面这个排序：</p>
<p>72 8 88 72 77 42 17 98 50 17 98 67 39 77 42 17 98</p>
<p>那我们去p=4 ,我们取出来的hash值全部都来自头部的hash, 假设这个文章是有很多段组成的，我们这样选取，很可能使得我们的特征分布不均匀。这样我们很可能只提取到了某些段落的特征值，而完全的忽略了某些段落。</p>
<p>要是我们忽略的段落正好是我们抄袭的段落，那么我们很可能就完全检测不到了。所以，你可以看到这个方法的缺点了吧。有时候取余选取的特征值存在分配不均匀的情况。为了改善这个情况，我们就要推出我们今天的主角 winnowing.</p>
<h1 id="Winnowing方法"><a href="#Winnowing方法" class="headerlink" title="Winnowing方法"></a>Winnowing方法</h1><p>为了使得我们的选取的特征值分布相对来说比较合理一点，这里我们使用了winnowing 的方法。在上面的一个小节中我们已经对这个方法做了一个简单的介绍。</p>
<p>这个方法的基本思想就是，我们首先设置一个大小为W的滑动窗口。将每个窗口中最小的那个hash保留下来（如果窗口中最小的hash 有两个或者多个，就保留最右边的那一个），这样就保证了我们保留下来的文档原文的间隔不会超过 W+K-1.注意，选过的特征是不能在挑选的，我们需要记录下下标。因为hash值是可能重复的。</p>
<p>为什么我们能保证间隔就是 W+K-1？why? 你想过这个问题了没有？</p>
<p>好的我们现在再来解答一下这个问题： 我们的特征值如下：</p>
<p><img src="/wp-content/uploads/2017/01/Winnowing_sample-512x43.png" alt=""></p>
<p>我们假设我们选取的窗口的大小为4：</p>
<p><img src="/wp-content/uploads/2017/01/Winnowing_windows.png" alt=""></p>
<p>在第一个窗口中我们的最小值是A3:8，以此类推</p>
<p><img src="/wp-content/uploads/2017/01/Winnowing_windows_step_1-512x70.png" alt=""></p>
<p>在第二个窗口中我们的最小值依然是A3:8</p>
<p><img src="/wp-content/uploads/2017/01/Winnowing_windows_step_2-512x60.png" alt=""></p>
<p>在第三个窗口中我们的最小值依然是A3:8</p>
<p><img src="/wp-content/uploads/2017/01/Winnowing_windows_step_3-512x64.png" alt=""></p>
<p>在第四个窗口中我们的最小值依然是A3:8</p>
<p><img src="/wp-content/uploads/2017/01/Winnowing_windows_step_4-512x67.png" alt=""></p>
<p>直到第五个窗口的时候我们才能加入新的特征值</p>
<p><img src="/wp-content/uploads/2017/01/Winnowing_windows_step_5-512x60.png" alt=""></p>
<p>我们可以看到最差的情况就是上面我们说的这情况，其实就是W-1</p>
<p>按照这种方法我们可以选举出我们的所有特征值。</p>
<p>假设我们的hash值的集合是下面这个样子</p>
<p><img src="/wp-content/uploads/2017/01/hash_set.png" alt=""></p>
<p>我们设置窗口的大小为4：</p>
<p><img src="/wp-content/uploads/2017/01/Winnowing_windows_matrix.png" alt=""></p>
<p>我们得到最终的特征值就是下面这个样子：</p>
<p><img src="/wp-content/uploads/2017/01/Fingerprint.png" alt=""></p>
<p>前面的是特征值，后面的是特征值对应的下标。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://kidozh.github.io/2017/01/23/e8-a7-a3-e5-86-b3python-e5-ae-89-e8-a3-85-e5-8c-85-e6-97-b6-e5-80-99-e7-9a-84error-unable-to-find-vcvarsall-bat-e9-97-ae-e9-a2-98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="kido zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="kidozh">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/01/23/e8-a7-a3-e5-86-b3python-e5-ae-89-e8-a3-85-e5-8c-85-e6-97-b6-e5-80-99-e7-9a-84error-unable-to-find-vcvarsall-bat-e9-97-ae-e9-a2-98/" itemprop="url">解决Python安装包时候的error: Unable to find vcvarsall.bat问题</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-01-23T07:13:38+08:00">
                2017-01-23
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>现在Windows开发Python，需要装一个名为<code>hcluster</code>的包，然后就开始报<a href="http://stackoverflow.com/questions/2817869/error-unable-to-find-vcvarsall-bat" target="_blank" rel="external">error: Unable to find vcvarsall.bat</a>的错误。stackoverflow上面已经讲的很清楚了，所以这就只是一个翻译贴了。</p>
<h1 id="安装Microsoft-Visual-C-Compiler-for-Python-2-7"><a href="#安装Microsoft-Visual-C-Compiler-for-Python-2-7" class="headerlink" title="安装Microsoft Visual C++ Compiler for Python 2.7"></a>安装Microsoft Visual C++ Compiler for Python 2.7</h1><p>地址在这里：<a href="http://www.microsoft.com/en-us/download/details.aspx?id=44266" target="_blank" rel="external">http://www.microsoft.com/en-us/download/details.aspx?id=44266</a></p>
<p>这样你就直接运行就可以了。需要注意的事情，你必须合理的设置环境变量的值以确保其能正常使用。</p>
<p>按照这样来说也应该没问题了，但是很不幸的，也可能出差错。</p>
<p>注意，在正常情况下，你需要避免下项发生。</p>
<h1 id="配置Visual-Studio的环境变量"><a href="#配置Visual-Studio的环境变量" class="headerlink" title="配置Visual Studio的环境变量"></a>配置Visual Studio的环境变量</h1><p>在使用setup.py安装程序包时，Python 2.7就会搜索已安装的Visual Studio。所以假设你已经安装了VS，那么在上述情况不成功的时候，你还需要在运行python setup.py install的之前，需要设定临时的环境变量。</p>
<p>设置的环境变量应该与你的VS版本有关。</p>
<ul>
<li>Visual Studio 2010 (VS10): <code>SET VS90COMNTOOLS=%VS100COMNTOOLS%</code></li>
<li>Visual Studio 2012 (VS11): <code>SET VS90COMNTOOLS=%VS110COMNTOOLS%</code></li>
<li>Visual Studio 2013 (VS12): <code>SET VS90COMNTOOLS=%VS120COMNTOOLS%</code></li>
<li>Visual Studio 2015 (VS14): <code>SET VS90COMNTOOLS=%VS140COMNTOOLS%</code></li>
</ul>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>更多的答案，你可以直接查阅<a href="http://stackoverflow.com/questions/2817869/error-unable-to-find-vcvarsall-bat" target="_blank" rel="external">StackOverFlow</a>关于这个问题的答案。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://kidozh.github.io/2016/12/30/e4-bd-bf-e4-bd-a0-e7-9a-84-e4-b8-bb-e9-a2-98-e6-9b-b4-e5-8a-a0-e5-ae-89-e5-85-a8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="kido zhang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="kidozh">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2016/12/30/e4-bd-bf-e4-bd-a0-e7-9a-84-e4-b8-bb-e9-a2-98-e6-9b-b4-e5-8a-a0-e5-ae-89-e5-85-a8/" itemprop="url">使你的主题更加安全</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-12-30T05:40:24+08:00">
                2016-12-30
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/网页/" itemprop="url" rel="index">
                    <span itemprop="name">网页</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>我们正准备要开始创建我们主题的模板文件了。在我们做这之前，快速了解数据验证对于WordPress的重要性是非常必要的。</p>
<h1 id="为什么主题的安全性很重要？"><a href="#为什么主题的安全性很重要？" class="headerlink" title="为什么主题的安全性很重要？"></a>为什么主题的安全性很重要？</h1><p>下面来自WordPress Codex的话很好的回答了这个问题：</p>
<blockquote>
<p>Untrusted data comes from many sources (users, third party sites, your own database!, …) and all of it needs to be validated both on input and output.</p>
<p>“来自许多不可信的来源（例如用户、第三方站点甚至是你自己的数据库）的输入和输出都应该被验证”<br>我们这里需要假设所有在你WordPress数据库中输入和输出的数据都是不安全的，并且需要根据适用的环境来对数据和内容进行认证。例如，我们不希望在设置页面上的文本框中输入的HTML代码，作为主题文件中的作为HTML运行，因为这可能会破坏我们的布局。 更糟糕的是，如果动态内容的代码是JavaScript或SQL查询，那么您的站点可能会面临<a href="http://en.wikipedia.org/wiki/Cross-site_scripting" target="_blank" rel="external">跨站点脚本攻击</a>（XSS）攻击或<a href="http://en.wikipedia.org/wiki/SQL_injection" target="_blank" rel="external">SQL注入</a>的风险。</p>
</blockquote>
<p>WordPress提供了一系列的函数，这些函数能让我们使得我们的数据变得安全，这些函数能帮助你：</p>
<ol>
<li>转义一些诸如单双引号、&amp;、大小于符号特殊字符到他们等价的符号(&quot;, &lt;, &gt;, etc)，所以他们不可能作为代码运行。这也被成为输出验证以及转义</li>
<li>确保你输入到数据库之中的内容和你所想的一致（举个例子，检查文字框包含了不含有HTML标签的安全文字）这也被称为输入验证。<br>在这个指南之中，我们将会关注第一个，转义数据。</li>
</ol>
<p>第二项也是对于主题搜集来自用户的数据来说非常重要，比如主题选项页面，主题设置。当然这一部分已经超出了这本章的所述范围了。</p>
<h1 id="输出转义以及无害化"><a href="#输出转义以及无害化" class="headerlink" title="输出转义以及无害化"></a>输出转义以及无害化</h1><p>我们的首要无害化代码的武器就是<a href="http://codex.wordpress.org/Function_Reference/esc_attr" target="_blank" rel="external">esc_attr()</a>和<a href="http://codex.wordpress.org/Function_Reference/esc_attr_e" target="_blank" rel="external">esc_attr_e()</a> 这两个函数，我们将会在此后使用多次，到时候用的时候我会告诉你们他们究竟是什么。</p>
<p>当我们输出HTML内置的属性的时候，这两个函数都会转义我们上面说的字符（这些字符可能会被错误的解析为代码）。<a href="http://codex.wordpress.org/Function_Reference/esc_attr" target="_blank" rel="external">esc_attr()</a>是用于在PHP内部转义代码而<a href="http://codex.wordpress.org/Function_Reference/esc_attr_e" target="_blank" rel="external">esc_attr_e()</a>则会在屏幕上显示我们正在转义的代码。</p>
<p>这里就有一个活生生的例子，这个代码我们将会在之后索引页的教程之中使用到这些函数。<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;h1 class="entry-title"&gt;&lt;a href="&lt;?php the_permalink(); ?&gt;" title="&lt;?php echo esc_attr( sprintf( __( 'Permalink to %s', 'shape' ), the_title_attribute( 'echo=0' ) ) ); ?&gt;" rel="bookmark"&gt;&lt;?php the_title(); ?&gt;&lt;/a&gt;&lt;/h1&gt;</div></pre></td></tr></table></figure></p>
<p>这个代码就会显示博文的标题。即使你不知道现在做的是什么，注意一下，我们在<code>&lt;a&gt;</code>标签之中，包括了title的标签以及title里面的值esc_attr()。所有包含在HTML属性标签都被认为是不安全的。所以<code>&lt;?php echo esc_attr( sprintf( __( &#39;Permalink to %s&#39;, &#39;book&#39; ), the_title_attribute( &#39;echo=0&#39; ) ) ); ?&gt;</code>也能包含所有的事情，当然也包含了隐含不安全字符的的内容。</p>
<p>深入这个教程，我们将会看到更多的例子。当然，如果你还想深入了解数据无害化以及验证的话，你也可以查阅Stephen Harris的<a href="http://wp.tutsplus.com/tutorials/creative-coding/data-sanitization-and-validation-with-wordpress/" target="_blank" rel="external">Data Validation and Sanitization With WordPress</a> 。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="kido zhang" />
          <p class="site-author-name" itemprop="name">kido zhang</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">112</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">19</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">78</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/kidozh" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/331837926Ji" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                    
                      Twitter
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/kidozh" target="_blank" title="weibo">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      weibo
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://douban.com/people/kidozh" target="_blank" title="douban">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      douban
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.zhihu.com/people/kidozh" target="_blank" title="zhihu">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      zhihu
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2014 &mdash; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">kido zhang</span>

  
</div>


  <div class="powered-by">
    由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
  </div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">
    主题 &mdash;
    <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
      NexT.Pisces
    </a>
  </div>


        







        
      </div>
    </footer>

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  








  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
